{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81023720",
   "metadata": {},
   "source": [
    "### moving from jinja to python.py prompts \n",
    "i though i will use jinja for prompts here but forget that  jinja is not made for routing , my prompts \n",
    "here looks less like template and more like dynamic routing prompts \n",
    "\n",
    " i was using lots and lots of if else condition \n",
    "say for 20 agent , checking 20 if else condition make system slow so i am moving \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6abefb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"    \n",
    "    Returns the prompt text for a given role and agent type.\n",
    "    Fills in any required variables provided as kwargs.\n",
    "    \n",
    "    Parameters:\n",
    "        role (str): 'system' or 'user'\n",
    "        agent_type (str): e.g., 'validator_agent', 'researcher_agent'\n",
    "        **kwargs: required variables for the prompt (if any)\n",
    "        \n",
    "    Returns:\n",
    "        str: Fully rendered prompt text\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from prompt import get_prompt\n",
    "from llm_clients import call_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fed6f",
   "metadata": {},
   "source": [
    "## create an MCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9b20a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_1\n",
      "Hello, this is a test message.\n",
      "{'priority': 'high', 'parents': ['writer-agent'], 'task_id': '6cdf83af-6735-46d1-a3cf-c5d6e7379c3e'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "from pydantic import BaseModel , field_validator , Field \n",
    "from typing import Dict, Any , Union , TypedDict\n",
    "import uuid\n",
    "\n",
    "\n",
    "class ValidatorContext(BaseModel):\n",
    "    task: str\n",
    "    source_summary: str\n",
    "    draft_post: str\n",
    "\n",
    "\n",
    "\n",
    "class MCPMessage(BaseModel):\n",
    "    protocol_version: str = \"1.0\"\n",
    "    sender: str= Field(min_length=3)\n",
    "    content: Any\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "def create_mcp_message(sender: str ,  content: Any, metadata=None) -> MCPMessage:\n",
    "    \"\"\"Create a message in MCP format.\"\"\"\n",
    "\n",
    "    \n",
    "    if not isinstance(content, (str, dict, ValidatorContext)):\n",
    "        raise TypeError(\n",
    "            f\"content must be str, dict, or ValidatorContext; got {type(content).__name__}\"\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    metadata = metadata or {} \n",
    "    metadata.setdefault(\"task_id\", str(uuid.uuid4()))\n",
    "    metadata.setdefault(\"parents\",[])\n",
    "\n",
    "    return MCPMessage(\n",
    "        sender=sender,\n",
    "        content=content,\n",
    "        metadata=metadata   \n",
    "    )\n",
    "\n",
    "mcp_answer = create_mcp_message(\"agent_1\", \"Hello, this is a test message.\", {\"priority\": \"high\" , \"parents\": [\"writer-agent\"]})\n",
    "print(mcp_answer.sender)\n",
    "print(mcp_answer.content)\n",
    "print(mcp_answer.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d30973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MCP Message (Sender: agent_1) ---\n",
      "Content: Hello, this is a test message.\n",
      "Metadata Keys: ['priority', 'parents', 'task_id']\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from helper import display_mcp \n",
    "display_mcp(mcp_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a8b57",
   "metadata": {},
   "source": [
    "####  not using any fallback logics or anything or simplicity \n",
    "#### i found i  will only use those if topic specific or  in end to end projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb6cf0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_context_librarian(mcp_message:MCPMessage):\n",
    "    \"\"\"\n",
    "        Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if \"intent_query\" not in mcp_message.content:\n",
    "        raise KeyError(\"mcp_message['content'] must contain 'intent_query'\")\n",
    "\n",
    "        \n",
    "    print(\"\\\\n[Librarian] Activated. Analyzing intent...\")\n",
    "    requested_intent = mcp_message.content['intent_query']\n",
    "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
    "    if results:\n",
    "        match = results[0]\n",
    "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score:{match['score']:.2f})\")\n",
    "        blueprint_json = match['metadata']['blueprint_json']\n",
    "        content = {\"blueprint\": blueprint_json}\n",
    "\n",
    "    else:\n",
    "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
    "        content = {\"blueprint\": json.dumps({\"instruction\": \"Generate the content neutrally.\"})}\n",
    "    return create_mcp_message(\"Librarian\", content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b60ec62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent_query': 'informal greeting style',\n",
       " 'topic_query': 'greeting conventions'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef9009",
   "metadata": {},
   "source": [
    "## research agent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "753121c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pine cone query \n",
    "from helper import query_pinecone \n",
    "## takes  query_text , namespaces and tok k (optional, because defult is 1)\n",
    "## and return index query ... response['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d24723db",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'genai-mas-mcp-01'\n",
    "NAMESPACE_KNOWLEDGE = \"knowledgeStore\"\n",
    "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b0766b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_or_create_index\n",
    "from llm_clients import pine_cone_client\n",
    "\n",
    "index = get_or_create_index(\n",
    "    pc=pine_cone_client,\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding_dim=384,\n",
    "    namespaces_to_clear=None  # important: keep existing vectors\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c49f7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define test query\n",
    "query_text = \"create a suspense narrator story\"\n",
    "\n",
    "# 3. Query CONTEXT namespace (semantic blueprints live here)\n",
    "context_results = query_pinecone(\n",
    "    index=index,\n",
    "    query_text=query_text,\n",
    "    namespace=NAMESPACE_KNOWLEDGE,\n",
    "    top_k=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7720707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'knowledge_chunk_1',\n",
       "  'metadata': {'text': 'nuclear power, demonstrating that solar energy can '\n",
       "                       'support missions even at great distances from the Sun.  '\n",
       "                       'Mars exploration represents another major focus of '\n",
       "                       'modern space science. Robotic rovers act as mobile '\n",
       "                       'laboratories, analyzing soil, rocks, and atmospheric '\n",
       "                       'conditions. NASA’s Perseverance rover is specifically '\n",
       "                       'designed to investigate whether Mars ever supported '\n",
       "                       'microbial life, while also testing technologies for '\n",
       "                       'future human missions. It carried the Ingenuity '\n",
       "                       'helicopter, which achieved the first powered flight on '\n",
       "                       'another planet, proving that aerial exploration is '\n",
       "                       'possible beyond Earth.  Together, these missions show '\n",
       "                       'how space exploration has evolved from political '\n",
       "                       'competition into a long-term scientific and '\n",
       "                       'technological endeavor aimed at expanding human '\n",
       "                       'knowledge and enabling future interplanetary travel.'},\n",
       "  'score': 0.00999168586,\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c738d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nuclear power, demonstrating that solar energy can support missions even at great distances from the Sun.  Mars exploration represents another major focus of modern space science. Robotic rovers act as mobile laboratories, analyzing soil, rocks, and atmospheric conditions. NASA’s Perseverance rover is specifically designed to investigate whether Mars ever supported microbial life, while also testing technologies for future human missions. It carried the Ingenuity helicopter, which achieved the first powered flight on another planet, proving that aerial exploration is possible beyond Earth.  Together, these missions show how space exploration has evolved from political competition into a long-term scientific and technological endeavor aimed at expanding human knowledge and enabling future interplanetary travel.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text['metadata']['text'] for text in context_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a25286",
   "metadata": {},
   "source": [
    "## agent research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "782474c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_clients import cohere_chat_client\n",
    "from prompt import get_prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d1282a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_researcher(mcp_message:MCPMessage):\n",
    "    \n",
    "    topic= mcp_message.content['topic_query']\n",
    "    results= query_pinecone (topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
    "    if not result: \n",
    "        print(\"[researcher] no Relevant information found\")\n",
    "        return create_mcp_message(\"Researcher\", {\"facts\" : \"No data found\"})\n",
    "    \n",
    "    source_texts = [match['metadata']['text'] for match in results]\n",
    "    system_prompt= get_prompt(role=\"system\", agent_type=\"research_synthesis_ai\")\n",
    "    \n",
    "    user_prompt=  get_prompt(role=\"user\", agent_type=\"research_synthesis_ai\",topic= topic , sources= source_texts )\n",
    "    \n",
    "    findings= call_llm(system_prompt, user_prompt)\n",
    "    \n",
    "    return create_mcp_message(\"Researcher\" , {\"facts\": findings})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3673764",
   "metadata": {},
   "source": [
    "## Writer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e3d0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_writer(mcp_message):\n",
    "    \n",
    "    print(\"\\\\n[Writer] Activated. Applying blueprint to facts...\")\n",
    "    facts= mcp_message.content['facts']\n",
    "    blueprint_json_string= mcp_message.content['blueprint']\n",
    "    \n",
    "    system_prompt= get_prompt(role='system' , agent_type=\"writer_blueprint\" , blueprint_json_string = blueprint_json_string)\n",
    "    user_prompt= get_prompt(role=\"user\",agent_type= \"research_facts\", facts= facts  )\n",
    "    \n",
    "    final_output= call_llm(system_prompt, user_prompt)\n",
    "    return create_mcp_message(\"Writer\", {\"output\": final_output})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56704326",
   "metadata": {},
   "source": [
    "# final (building the orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e1fddd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an expert goal analyst.\\nAnalyze the user's high-level goal and extract two components:\\n1. 'intent_query': A descriptive phrase summarizing the desired style, tone, or format, optimized for searching a context library (e.g., 'suspenseful narrative blueprint', 'objective technical explanation structure').\\n2. 'topic_query': A concise phrase summarizing the factual subject matter required (e.g., 'Juno mission objectives and power', 'Apollo 11 landing details').\\nRespond ONLY with a JSON object containing these two keys.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompt import get_prompt \n",
    "system_prompt= get_prompt(role='system' , agent_type=\"goal_analyst\" )\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65348e",
   "metadata": {},
   "source": [
    "## Goal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb46e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_clients import call_llm\n",
    "analysis_system_prompt = \"\"\"You are an expert goal analyst.\n",
    "                            Analyze the\n",
    "                            user's high-level goal and extract two components:\n",
    "                            1. 'intent_query': A descriptive phrase summarizing the desired style, tone,\n",
    "                            or format, optimized for searching a context library (e.g., \"suspenseful narrative\n",
    "                            blueprint\", \"objective technical explanation structure\").\n",
    "                            2. 'topic_query': A concise phrase summarizing the factual subject matter\n",
    "                            required (e.g., \"Juno mission objectives and power\", \"Apollo 11 landing details\").\n",
    "                            Respond ONLY with a JSON object containing these two keys.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b90cab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResearcherOutput(intent_query='informal greeting structure', topic_query='common English salutations')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer= call_llm(system_prompt = analysis_system_prompt ,user_content=  \"hellow\", structured=True)\n",
    "answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bf1b466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent_query': 'informal greeting structure',\n",
       " 'topic_query': 'common English salutations'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2ad1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e831dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(hight_level_goal):\n",
    "    \"\"\" Manages the workflow of the context Aware MAS \"\"\" \n",
    "    \n",
    "    print(f\"high level goal :  {hight_level_goal}\")\n",
    "    system_prompt= get_prompt(role='system' , agent_type=\"goal_analyst\" )\n",
    "    \n",
    "    analysis_results= call_llm(system_prompt, hight_level_goal, structured=True)\n",
    "    \n",
    "    try: \n",
    "\n",
    "        intent_query= analysis_results.intent_query \n",
    "        topic_query= analysis_results.topic_query\n",
    "    except   : \n",
    "        return  \"error occours while trying to fetch intent , topic query\"\n",
    "    \n",
    "    # Agent coordination \n",
    "    mcp_to_librarian= create_mcp_message(\n",
    "        sender= \"Orchestration\",\n",
    "        content= {\"intent_query\" : intent_query} \n",
    "    )\n",
    "    \n",
    "    mcp_from_librarian= agent_context_librarian(mcp_to_librarian)\n",
    "    \n",
    "    ## display mcp \n",
    "    display_mcp(mcp_from_librarian ,\"Librarian -> Orchestrator\")\n",
    "    \n",
    "    context_blueprint = mcp_from_librarian.content.get('blueprint')\n",
    "    \n",
    "    if not context_blueprint:\n",
    "        raise ValueError(\"No blueprint found in Librarian's content\")\n",
    "\n",
    "    ## step 2 \n",
    "    mcp_to_researcher = create_mcp_message( sender=\"Orchestrator\", content={\"topic_query\": topic_query})\n",
    "    mcp_from_researcher=agent_researcher(mcp_to_researcher)\n",
    "    display_mcp(mcp_from_researcher)\n",
    "    \n",
    "    research_finding= mcp_from_researcher.content.get('facts')\n",
    "    if not research_finding :\n",
    "        raise ValueError(\"no research finding \")\n",
    "    \n",
    "    ## generate final output , step 3\n",
    "    writer_task = {\n",
    "                        \"blueprint\": context_blueprint,\n",
    "                        \"facts\": research_finding\n",
    "                        }\n",
    "    \n",
    "    mcp_to_writer = create_mcp_message(\n",
    "                                        sender=\"Orchestrator\",\n",
    "                                        content=writer_task\n",
    "                                        )\n",
    "    \n",
    "                                            \n",
    "    mcp_from_writer = agent_writer(mcp_to_writer)\n",
    "    display_mcp(mcp_from_writer, \"Writer -> Orchestrator\")\n",
    "    \n",
    "    \n",
    "    final_result = mcp_from_writer.content.get('output')\n",
    "    \n",
    "    return final_result \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ac97e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high level goal :  Explain space exploration in a suspenseful story\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'MCPMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43morchestrator\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain space exploration in a suspenseful story\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36morchestrator\u001b[39m\u001b[34m(hight_level_goal)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Agent coordination \u001b[39;00m\n\u001b[32m     17\u001b[39m mcp_to_librarian= create_mcp_message(\n\u001b[32m     18\u001b[39m     sender= \u001b[33m\"\u001b[39m\u001b[33mOrchestration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     content= {\u001b[33m\"\u001b[39m\u001b[33mintent_query\u001b[39m\u001b[33m\"\u001b[39m : intent_query} \n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m mcp_from_librarian= \u001b[43magent_context_librarian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcp_to_librarian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m## display mcp \u001b[39;00m\n\u001b[32m     25\u001b[39m display_mcp(mcp_from_librarian ,\u001b[33m\"\u001b[39m\u001b[33mLibrarian -> Orchestrator\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36magent_context_librarian\u001b[39m\u001b[34m(mcp_message)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magent_context_librarian\u001b[39m(mcp_message:MCPMessage):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m        Retrieves the appropriate Semantic Blueprint from the Context Library.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mintent_query\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmcp_message\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmcp_message[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m\u001b[33m] must contain \u001b[39m\u001b[33m'\u001b[39m\u001b[33mintent_query\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn[Librarian] Activated. Analyzing intent...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'MCPMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result = orchestrator(\"Explain space exploration in a suspenseful story\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ee131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
